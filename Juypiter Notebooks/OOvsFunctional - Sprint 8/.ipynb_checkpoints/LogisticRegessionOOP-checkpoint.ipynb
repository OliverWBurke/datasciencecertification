{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x):\n",
    "    return np.where(x==0, np.log(x+1e-10), np.log(x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self,df,alpha): \n",
    "        self.n_iterations=1000        \n",
    "        # assign the input arguments to class attributes\n",
    "        # INSERT CODE HERE\n",
    "        \n",
    "        self.X = df[df.columns.difference(['target'])]\n",
    "        self.y = df.target\n",
    "        self.n_features = len(list(self.X))\n",
    "        self.alpha = (alpha)\n",
    "        self.w = self._initialise_w(self.n_features)\n",
    "        # \"private\" methods are preceded with an underscore in the name\n",
    "        # in other programming languages such functions are treated differently\n",
    "        # in Python we use this convention as an indication to the user that they are\n",
    "        # intended to be called by other methods in the class and not from outside of the class\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Implements the sigmoid function\n",
    "                           1\n",
    "        sigmoid(x) =  -------------\n",
    "                      1 + exp(-x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float or iterable\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or iterable\n",
    "        \"\"\"\n",
    "        return 1./(1 + np.exp(-x))\n",
    "        \n",
    "    def _calc_dJ_dw(self, X, y, w):\n",
    "        '''Function to calculate dJ/dw\n",
    "\n",
    "        Parameters:\n",
    "        X : 2d-array, feature matrix\n",
    "        y : 1d-array, true y values\n",
    "        w : 1d-array, linear regression model weights\n",
    "\n",
    "        Returns:\n",
    "        dJ_dw : 1d-array\n",
    "        '''\n",
    "        m = len(y)\n",
    "        ypred = self.predict(X, w)\n",
    "        dJ_dw = np.dot(X.T, (ypred - y)) / y.shape[0]\n",
    "        # Check that dJ_dw has the right shape\n",
    "        assert dJ_dw.shape == w.shape\n",
    "        return dJ_dw\n",
    "    \n",
    "    def _initialise_w(self, n_features):\n",
    "        '''Initialise the weights vector w with random values.\n",
    "\n",
    "        Parameters:\n",
    "        n_features : int, number of features \n",
    "\n",
    "        Returns:\n",
    "        w : 1d-array\n",
    "        '''\n",
    "        # Set a seed so we get predictable values\n",
    "        np.random.seed(1)\n",
    "        # create a random numpy array of features of length n_features\n",
    "        self.w = np.random.rand(n_features,)\n",
    "        # Check that w has the right shape\n",
    "        assert self.w.shape == (n_features,)\n",
    "        return self.w\n",
    "    \n",
    "    def _update_w(self, w, alpha, dJ_dw):\n",
    "        '''Update the weights vector w.\n",
    "\n",
    "        Parameters:\n",
    "        w : 1d-array, weights vector\n",
    "        alpha : float, learning rate\n",
    "        dJ_dw : 1d-array, gradients vector\n",
    "\n",
    "        Returns:\n",
    "        new_w : 1d-array, updated weights vector\n",
    "        '''    \n",
    "        new_w = w - (alpha * dJ_dw )\n",
    "        # Check the dimensions of new_w\n",
    "        assert new_w.shape == w.shape\n",
    "        return new_w\n",
    "    \n",
    "    def cost_function(self, y, ypred):\n",
    "        '''The cost function J(w) as defined in Equation (4).\n",
    "\n",
    "        Parameters:\n",
    "        ypred : 1d-array, y values predicted by model \n",
    "        y : 1d-array, true y values\n",
    "\n",
    "        Returns:\n",
    "        float, J (the cost)\n",
    "        '''\n",
    "        # m is the number of samples\n",
    "        m = y.shape[0]\n",
    "\n",
    "        # J is the cost\n",
    "        J = (-1 / m ) * sum(y * safe_log(ypred) + (1 - y) * safe_log(1 - ypred)) \n",
    "\n",
    "        # Check that J is a scalar\n",
    "        #assert J.shape == ()\n",
    "\n",
    "        return J\n",
    "    def predict_proba(self, y_pred):\n",
    "        # INSERT CODE HERE\n",
    "        return 1\n",
    "    def predict(self, X, w):\n",
    "        '''The logistic regression model as defined in Equation (1).\n",
    "        It takes X and w as inputs and returns a 1d-array of predictions.\n",
    "\n",
    "        Parameters:\n",
    "        X : 2d-array, shape=(n_samples,n_features)\n",
    "        w : 1d-array, shape=(n_features,)\n",
    "\n",
    "        Returns:\n",
    "        ypred : 1d-array, shape=(n_samples,)\n",
    "        '''\n",
    "        # Check that the number of features in X is equal to the number features in the weights vector\n",
    "        assert X.shape[1] == len(w)\n",
    "        ypred = self._sigmoid(np.dot(X,w))\n",
    "        # Check that the number of predictions made is equal to the number of samples in X\n",
    "        assert len(ypred) == X.shape[0]\n",
    "        return ypred\n",
    "\n",
    "    def fit(self, verbose=False ):#, X, y, n_iterations=100, alpha=0.01, verbose=False):\n",
    "        # INSERT CODE HERE\n",
    "        '''Fit linear regression model to data X, y.\n",
    "\n",
    "        Parameters:\n",
    "        X : 2d-array, feature matrix shape=(m, n_features)\n",
    "        y : 1d-array, targets\n",
    "        n_iterations : int, number of iterations of gradient descent\n",
    "        alpha : float, learning rate\n",
    "        verbose : bool, prints the cost every 10 iterations\n",
    "\n",
    "        Returns:\n",
    "        w : nd-array, final weights matrix shape=(n_features,)\n",
    "        cost_values : 1d-array, cost at each iteration shape=(n_iterations,)\n",
    "        w_values : nd-array, weights at each iteration shape=(n_iterations, n_features)\n",
    "        '''\n",
    "\n",
    "        # We are going to save the values of the cost and w at each iteration for later analysis\n",
    "        cost_values = [] \n",
    "        w_values = [] \n",
    "\n",
    "        # Repeat n_iterations times\n",
    "        for i in range(self.n_iterations):\n",
    "\n",
    "            # Step 2: Calculate the gradient \n",
    "            dJ_dw = self._calc_dJ_dw(self.X, self.y, self.w)\n",
    "\n",
    "            # Step 3: Update w\n",
    "            w = self._update_w(self.w, self.alpha, dJ_dw)# INSERT CODE HERE\n",
    "\n",
    "            # Calculate the cost \n",
    "            cost = self.cost_function(self.predict(self.X, self.w), self.y)# INSERT CODE HERE\n",
    "\n",
    "            if verbose and i % 100 == 0:\n",
    "                print('Iteration {}: Cost={:.6f}'.format(i, cost))\n",
    "\n",
    "            # Save the values of the cost and w after each iteration\n",
    "            cost_values.append(cost)\n",
    "            w_values.append(w)\n",
    "\n",
    "        cost_values = np.array(cost_values)\n",
    "        w_values = np.array(w_values)\n",
    "\n",
    "        return w, cost_values, w_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01\n",
      " 1.46755891e-01 9.23385948e-02 1.86260211e-01 3.45560727e-01\n",
      " 3.96767474e-01 5.38816734e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oliver\\Anaconda3\\envs\\DataScienceCertification\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01\n",
      " 1.46755891e-01 9.23385948e-02 1.86260211e-01 3.45560727e-01\n",
      " 3.96767474e-01 5.38816734e-01]\n",
      "Object `` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# Edit the commands below as required\n",
    "def get_data():\n",
    "    '''Load classification data for the exercise.\n",
    "    \n",
    "    Returns:\n",
    "    data : pandas DataFrame\n",
    "    '''\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=8, n_redundant=2, n_classes=2, random_state=42, n_clusters_per_class=1)\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=['feature_'+str(i) for i in range(10)])\n",
    "    df['target'] = y\n",
    "    return df\n",
    "df = get_data()\n",
    "# Initialse Model\n",
    "model1 = LogisticRegression(df,0.01)\n",
    "print(model1.w)\n",
    "# Fit Model\n",
    "fit = model1.fit()\n",
    "print(model1.w)\n",
    "# Make Predictions\n",
    "\n",
    "test_data = make_classification(n_samples=10, n_features=10, n_informative=8, n_redundant=2, n_classes=2, random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "\n",
    "??? = ???.predict(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

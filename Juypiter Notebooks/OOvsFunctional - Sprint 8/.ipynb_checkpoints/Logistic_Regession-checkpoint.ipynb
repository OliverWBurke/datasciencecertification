{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQhu31QFllAZ"
   },
   "source": [
    "# Logistic Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZ6b0FzGKITn"
   },
   "source": [
    "Aims:\n",
    "\n",
    "* Implement batch gradient descent for a logistic regression model\n",
    "* Review of cost function for classification problems\n",
    "* Review matrix dot products with `numpy`\n",
    "* Further practice with pandas and pyplot\n",
    "* Implementing a class in Python\n",
    "\n",
    "You will need to fill in parts that say \n",
    "    # INSERT CODE HERE\n",
    "    \n",
    "where possible the exercises will indicate the expected output for you to check your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xV-fKbeSgc5j"
   },
   "source": [
    "## Part A: Review of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-w2Tx2wliJg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29JmiAxbWpVi"
   },
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0ST6vQzl1CZ"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''Load classification data for the exercise.\n",
    "    \n",
    "    Returns:\n",
    "    data : pandas DataFrame\n",
    "    '''\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_informative=8, n_redundant=2, n_classes=2, random_state=42, n_clusters_per_class=1)\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=['feature_'+str(i) for i in range(10)])\n",
    "    df['target'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rGKDflLl6h3"
   },
   "outputs": [],
   "source": [
    "# Get the data. df is a pandas dataframe. \n",
    "df = get_data()\n",
    "X = df[df.columns.difference(['target'])]\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xPfKL4kogc5t",
    "outputId": "e5360019-2002-4abf-d7e9-97cc7966b40c"
   },
   "outputs": [],
   "source": [
    "# (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY1LobavKIT0"
   },
   "source": [
    "`df` is a DataFrame with 500 rows and 11 columns, including the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "yXEfb59mKIT1",
    "outputId": "c28d696d-a1f5-4c16-867f-6be23b42960c"
   },
   "outputs": [],
   "source": [
    "# Examine the target column\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA20ZOAYgc53"
   },
   "source": [
    "In this binary classification dataset we have 502 positive examples and 498 negative examples.\n",
    "\n",
    "**Question:** Is this a balanced dataset? What precautions do we have to take if a classification dataset is not balanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGNQTwlQgc53"
   },
   "source": [
    "**INSERT ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afTmi0cJKIT5"
   },
   "source": [
    "## 2. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMUkdDJeKIT5"
   },
   "source": [
    "Let's begin by taking a look at the first few rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uk9ds9KTKITy"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DT3oIOAKKIUB"
   },
   "source": [
    "As you will have previously seen, it is a good idea to make some simple plots of the various input features (where you are able!) to get a feel for the dataset. \n",
    "\n",
    "**Hints:**\n",
    "* Making more compact visualisations with subplots\n",
    "https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html\n",
    "\n",
    "* The seaborn package is also an excellent visualisation tool. Here are some examples of useful visuals for categorical data \n",
    "https://seaborn.pydata.org/tutorial/categorical.html#categorical-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0gvuWAu2Ek5"
   },
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWGZ3L1hqRgj"
   },
   "source": [
    "## 3. Defining the model and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83dbk3enoUQE"
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We will model the data using logistic regression.\n",
    "The predictions $\\hat{y}^i$ are given by the logistic model\n",
    "\n",
    "$\\hat{y}^i=\\sigma(\\sum_j w_j^{\\phantom{}}x^i_j)$,   $\\quad\\quad\\quad(1)$\n",
    "\n",
    "where\n",
    "\n",
    "$\\sigma(x)=1/(1+\\exp^{-x})$, $\\quad\\quad\\quad(2)$\n",
    "\n",
    "Notation explained:\n",
    "\n",
    "* The superscript $i$ denotes the $i$-th example (or row) in the dataset.\n",
    "* The subscript $j$ denotes the $j$-th feature (or column) in the dataset.\n",
    "* $w_j$ are the parameters of the model and indicate the dependence of $\\hat{y}$ on $x_j$.\n",
    "\n",
    "As a matrix dot product, we can write\n",
    "\n",
    "$\\hat{y}=\\sigma(X\\cdot\\mathbf{w})$,   $\\quad\\quad\\quad(2)$\n",
    "\n",
    "where $X$ is a matrix with shape `(n samples, n features)` and $\\mathbf{w}$ is the weights vector of length `n features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avSDpnt3KIUK"
   },
   "source": [
    "Define a `predict(X,w)` method that calculates the logistic regression model for a given input of X and w.\n",
    "\n",
    "It is useful to define a separate sigmoid function using the numpy `exp` method. The advantage of using the numpy implementation instead of `math.exp` is that it can perform an element-wise operation on all elements of an n-dimensional array.\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDtSFoAWgc6E"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yn5d18Khgc6G",
    "outputId": "c8f132ef-561c-4326-94eb-dbda1d3d111c"
   },
   "outputs": [],
   "source": [
    "# test function\n",
    "sigmoid(np.array([-0.5, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn2LcMpzgc6L"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    array([0.37754067, 0.5       , 0.73105858])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0b4bQp2LprH1"
   },
   "outputs": [],
   "source": [
    "# Define a function that calculates the linear regression model. This will come in useful later.\n",
    "def predict(X, w):\n",
    "    '''The logistic regression model as defined in Equation (1).\n",
    "    It takes X and w as inputs and returns a 1d-array of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    X : 2d-array, shape=(n_samples,n_features)\n",
    "    w : 1d-array, shape=(n_features,)\n",
    "\n",
    "    Returns:\n",
    "    ypred : 1d-array, shape=(n_samples,)\n",
    "    '''\n",
    "    # Check that the number of features in X is equal to the number features in the weights vector\n",
    "    assert X.shape[1] == len(w)\n",
    "\n",
    "    ypred = # INSERT CODE HERE\n",
    "    \n",
    "    # Check that the number of predictions made is equal to the number of samples in X\n",
    "    assert len(ypred) == X.shape[0]\n",
    "\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gXVf9JQDKIUQ",
    "outputId": "6618163e-9213-46a5-89e6-3fcb4b599bb8"
   },
   "outputs": [],
   "source": [
    "# Test your function with these example values of w\n",
    "w = np.linspace(0, 1, 10)\n",
    "predict(X, w)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3m-v47_KIUT"
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    array([0.45603552, 0.32943956, 0.8237889 , 0.46605423, 0.66157655])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxafyWDGKIUT"
   },
   "source": [
    "### Cost function\n",
    "In order to find the values of $\\mathbf{w}$ that best represent the data we need to define a cost function that quantifies how far away our estimate of $\\hat{y}$ differs from the true values $y$.\n",
    "\n",
    "For logistic regression the cost function to be minimised is the **cross-entropy** between the true $y^i$ and predicted $\\hat{y}^i$ values:\n",
    "\n",
    "$J(\\mathbf{w})=-\\frac{1}{m}\\sum^m_{i=1} \\left[y^i \\log \\hat{y}^i + (1-y^i) \\log(1-\\hat{y}^i)\\right]$. $\\quad\\quad\\quad(4)$\n",
    "\n",
    "* We sum the entropy over all samples from $i=1$ to $i=m$, where $m$ is the number of samples in the data.\n",
    "\n",
    "**Question:** Explain what each term of the loss function does.\n",
    "(a) how do mis-classified positive and negative examples increase the loss?\n",
    "(b) how do correctly classified positive and negative examples decrease the loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBK15a6mgc6V"
   },
   "source": [
    "**ANSWER:**\n",
    "\n",
    "    INSERT ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIFeo2S8KIUV"
   },
   "source": [
    "Define a `cost_function(ypred, y)` method that calculates the cost function in Equation (4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ibrCxAigc6X"
   },
   "outputs": [],
   "source": [
    "def safe_log(x):\n",
    "    return np.where(x==0, np.log(x+1e-10), np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63bxkO5pZmjk"
   },
   "outputs": [],
   "source": [
    "def cost_function(ypred, y):\n",
    "    '''The cost function J(w) as defined in Equation (4).\n",
    "\n",
    "    Parameters:\n",
    "    ypred : 1d-array, y values predicted by model \n",
    "    y : 1d-array, true y values\n",
    "\n",
    "    Returns:\n",
    "    float, J (the cost)\n",
    "    '''\n",
    "    # m is the number of samples\n",
    "    m = # INSERT CODE HERE\n",
    "    \n",
    "    # J is the cost\n",
    "    J = # INSERT CODE HERE\n",
    "    \n",
    "    # Check that J is a scalar\n",
    "    assert J.shape == ()\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "dOw489zrgc6b",
    "outputId": "9500966d-b56b-446f-c6dc-96f0ed9e16b8"
   },
   "outputs": [],
   "source": [
    "ypred = predict(X, w)\n",
    "cost_function(ypred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IuLOPCxNgc6d"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    1.404926243799119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT3vGbZbqV4F"
   },
   "source": [
    "## 4. Batch Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeODKjetp7m3"
   },
   "source": [
    "The goal of gradient descent is to find the values of $\\mathbf{w}$ that **minimise** the cost function $J$. \n",
    "\n",
    "The minimum is found by evaluating the _gradient_ $\\frac{dJ}{d\\mathbf{w}}$ at a given $\\mathbf{w}$ value, and taking a small step, the _learning rate_ $\\alpha$, against it.\n",
    "\n",
    "There are many different gradient descent algorithms. Here we will implement the **batch gradient descent algorithm**. In batch gradient descent, we will use all the examples in the sample at each iteration to update $w_0$ and $w_1$.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "1. Initialise $\\mathbf{w}$ with random values.\n",
    "2. Calculate the gradients $\\frac{dJ}{dw_i}$ at this point.\n",
    "3. Update the value of each parameter $\\mathbf{w}$ using the update equations\n",
    "\n",
    "  $w_i := w_i - \\alpha \\frac{dJ}{dw_i}$, $\\quad\\quad\\quad(5)$\n",
    "\n",
    "4. Repeat steps 2 and 3 until the change in $\\mathbf{w}$ is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYIuczBPqgeZ"
   },
   "source": [
    "Let's implement each step in turn.\n",
    "\n",
    "### Step 1: Initialise w with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3yNbgp4X6m8"
   },
   "outputs": [],
   "source": [
    "def initialise_w(n_features):\n",
    "    '''Initialise the weights vector w with random values.\n",
    "\n",
    "    Parameters:\n",
    "    n_features : int, number of features \n",
    "\n",
    "    Returns:\n",
    "    w : 1d-array\n",
    "    '''\n",
    "    # Set a seed so we get predictable values\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # create a random numpy array of features of length n_features\n",
    "    w = # INSERT CODE HERE\n",
    "    \n",
    "    # Check that w has the right shape\n",
    "    assert w.shape == (n_features,)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "-9BbDyWkYBK1",
    "outputId": "daa26457-88f4-4dab-bb4c-a5b92cc934a7"
   },
   "outputs": [],
   "source": [
    "# Test the initialise_w method\n",
    "w = initialise_w(X.shape[1])\n",
    "print('w = [{:.4f},{:.4f}]'.format(w[0], w[1]))\n",
    "\n",
    "ypred = predict(X, w)\n",
    "print('yhat[0]={:.4f}'.format(ypred[0]))\n",
    "\n",
    "cost = cost_function(ypred, y)\n",
    "print('Cost={:.4f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B57BEsyjibMO"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "w = [0.4170,0.7203]\n",
    "yhat[0]=0.8067\n",
    "Cost=0.7767\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P16h0KLqk4J"
   },
   "source": [
    "### Step 2: Calculate the gradient of J\n",
    "\n",
    "Recalling Equations (1) and (4):\n",
    "\n",
    "$\\hat{y}^i=\\sigma(\\sum_j w_j^{\\phantom{}}x^i_j)$,   $\\quad\\quad\\quad(1)$\n",
    "\n",
    "$J(\\mathbf{w})=-\\frac{1}{m}\\sum^m_{i=1} \\left[y^i \\log \\hat{y}^i + (1-y^i) \\log(1-\\hat{y}^i)\\right]$. $\\quad\\quad\\quad(4)$\n",
    "\n",
    "Evaluate the gradient $\\frac{dJ}{dw_i}$. You may need some pen and paper for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "TN_6Odvs21-w",
    "outputId": "a0f3cd6f-f6af-42e3-a5c5-25ffc987f8e6"
   },
   "outputs": [],
   "source": [
    "def calc_dJ_dw(X, y, w):\n",
    "    '''Function to calculate dJ/dw\n",
    "\n",
    "    Parameters:\n",
    "    X : 2d-array, feature matrix\n",
    "    y : 1d-array, true y values\n",
    "    w : 1d-array, linear regression model weights\n",
    "\n",
    "    Returns:\n",
    "    dJ_dw : 1d-array\n",
    "    '''\n",
    "    # Number of samples\n",
    "    m = len(y)\n",
    "    \n",
    "    ypred = # INSERT CODE HERE\n",
    "    \n",
    "    dJ_dw = # INSERT CODE HERE\n",
    "\n",
    "    # Check that dJ_dw has the right shape\n",
    "    assert dJ_dw.shape == w.shape\n",
    "\n",
    "    return dJ_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-1tUHT828Do"
   },
   "outputs": [],
   "source": [
    "# Check the calc_dJ_dw method\n",
    "dJ_dw = calc_dJ_dw(X, y, w)\n",
    "print('dJ/dw = [dJ/dw_0, dJ/dw_1] =', dJ_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuX4gZIVKIUu"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    dJ/dw = [dJ/dw_0, dJ/dw_1] = [ 0.49369713 -0.21468038 -0.31495645 -0.35295134  0.78093749  0.44299134\n",
    "    -0.35397491  0.04493079  0.22469626  0.34793172]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_8NAa40qoPq"
   },
   "source": [
    "### Step 3: Update the value of w\n",
    "\n",
    "With the gradients in hand we can update the value of $\\mathbf{w}$ using the update equations\n",
    "\n",
    "  $w_0 := w_0 - \\alpha \\frac{dJ}{dw_0}$, $\\quad\\quad\\quad(5)$\n",
    "\n",
    "  $w_1 := w_1 - \\alpha \\frac{dJ}{dw_1}$. $\\quad\\quad\\quad(6)$\n",
    "  \n",
    "This improves our estimate of $\\mathbf{w}$ by taking a step of size $\\alpha$ \"downhill\" towards the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lkrcNe-iCeI"
   },
   "outputs": [],
   "source": [
    "def update_w(w, alpha, dJ_dw):\n",
    "    '''Update the weights vector w.\n",
    "    \n",
    "    Parameters:\n",
    "    w : 1d-array, weights vector\n",
    "    alpha : float, learning rate\n",
    "    dJ_dw : 1d-array, gradients vector\n",
    "    \n",
    "    Returns:\n",
    "    new_w : 1d-array, updated weights vector\n",
    "    '''    \n",
    "    new_w = # INSERT CODE HERE\n",
    "    \n",
    "    # Check the dimensions of new_w\n",
    "    assert new_w.shape == w.shape\n",
    "    \n",
    "    return new_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Buj0_3ORr7Ys"
   },
   "source": [
    "Let's try taking one iteration of batch gradient descent. \n",
    "\n",
    "As a reminder, the algorithm goes as follows:\n",
    "1. Initialise $\\mathbf{w}=[w_0, w_1]$ with random values.\n",
    "2. Calculate the gradients $\\frac{dJ}{dw_0}$, $\\frac{dJ}{dw_1}$ at this point.\n",
    "3. Update the value of $\\mathbf{w}$ using the update equations $w_0 := w_0 - \\alpha \\frac{dJ}{dw_0}$, $w_1 := w_1 - \\alpha \\frac{dJ}{dw_1}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z52wGIe3YRum"
   },
   "outputs": [],
   "source": [
    "# Initialise w to random value\n",
    "w = # INSERT CODE HERE\n",
    "\n",
    "# Calculate the gradient\n",
    "dJ_dw = # INSERT CODE HERE\n",
    "\n",
    "# Set the value of alpha to 0.01\n",
    "alpha = 0.01\n",
    "\n",
    "# Update w\n",
    "new_w = # INSERT CODE HERE\n",
    "\n",
    "print('new w={}'.format(new_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd9Q3pyam_i9"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "new w=[0.41208503 0.7224713  0.00326394 0.30586209 0.13894652 0.08790868\n",
    " 0.18979996 0.34511142 0.39452051 0.53533742]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q8g7Aaw_YSHM"
   },
   "outputs": [],
   "source": [
    "# Calculate the cost at new_w\n",
    "\n",
    "new_cost = # INSERT CODE HERE\n",
    "\n",
    "print('Initial Cost: {:.4f}'.format(cost))\n",
    "print('Cost after one step of gradient descent: {:.4f}'.format(new_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOO300TvnEX5"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "TO UPDATE\n",
    "Initial Cost: 0.7767\n",
    "Cost after one step of gradient descent: 0.7607\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-o1u1HBxqvDu"
   },
   "source": [
    "### Step 4: Repeat\n",
    "\n",
    "We have a made a small improvement in the estimation of the parameters for our linear regression model. \n",
    "\n",
    "In order to find the best estimate of $\\mathbf{w}$ we will need to do many iterations of gradient descent to get to the minimum of $J(\\mathbf{w})$. \n",
    "\n",
    "Let's put everything together into a single \"fit\" function that takes X and y as inputs and iterates the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "228tEsDzY646"
   },
   "outputs": [],
   "source": [
    "def fit(X, y, n_iterations=100, alpha=0.01, verbose=False):\n",
    "    '''Fit linear regression model to data X, y.\n",
    "    \n",
    "    Parameters:\n",
    "    X : 2d-array, feature matrix shape=(m, n_features)\n",
    "    y : 1d-array, targets\n",
    "    n_iterations : int, number of iterations of gradient descent\n",
    "    alpha : float, learning rate\n",
    "    verbose : bool, prints the cost every 10 iterations\n",
    "    \n",
    "    Returns:\n",
    "    w : nd-array, final weights matrix shape=(n_features,)\n",
    "    cost_values : 1d-array, cost at each iteration shape=(n_iterations,)\n",
    "    w_values : nd-array, weights at each iteration shape=(n_iterations, n_features)\n",
    "    '''\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Step 1: Initialise w at a random point\n",
    "    w = # INSERT CODE HERE\n",
    "    \n",
    "    # We are going to save the values of the cost and w at each iteration for later analysis\n",
    "    cost_values = [] \n",
    "    w_values = [] \n",
    "    \n",
    "    # Repeat n_iterations times\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        # Step 2: Calculate the gradient \n",
    "        dJ_dw = # INSERT CODE HERE\n",
    "        \n",
    "        \n",
    "        # Step 3: Update w\n",
    "        w = # INSERT CODE HERE\n",
    "        \n",
    "        \n",
    "        # Calculate the cost \n",
    "        cost = # INSERT CODE HERE\n",
    "        \n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            print('Iteration {}: Cost={:.6f}'.format(i, cost))\n",
    "\n",
    "        # Save the values of the cost and w after each iteration\n",
    "        cost_values.append(cost)\n",
    "        w_values.append(w)\n",
    "  \n",
    "    cost_values = np.array(cost_values)\n",
    "    w_values = np.array(w_values)\n",
    "    \n",
    "    return w, cost_values, w_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHYQU6HUKIVA"
   },
   "source": [
    "Let's try fitting X and y with the values\n",
    "\n",
    "    alpha=0.01\n",
    "    n_iterations=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmp2XL43mKsx"
   },
   "outputs": [],
   "source": [
    "w, cost_values, w_values = fit(X, y, alpha=0.01, n_iterations=1000, verbose=True)\n",
    "\n",
    "print('Final value of w={}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAC3sLwgsQdH"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Final value of w=[-0.19853515  0.63897793  0.29965423  0.48473537 -0.26374862 -0.25969949\n",
    "  1.12662933  0.45179526 -0.48198174 -0.08320325]\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCxdTKW4gc6-"
   },
   "source": [
    "## 5. Visualising gradient descent\n",
    "\n",
    "### a. Cost as a function of gradient descent iteration\n",
    "\n",
    "It is often useful to plot the cost after each iteration of gradient descent to check that it is decreasing. (We will see in future weeks that there is a lot of other useful information here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXY6qhtWmMvy"
   },
   "outputs": [],
   "source": [
    "# Plot the cost at each iteration of gradient descent\n",
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fd-UMXe_1CX1"
   },
   "source": [
    "### b. Cost as a function of $\\mathbf{w}$\n",
    "\n",
    "Below is some code that will plot the cost function $J$ as a function of $w_0$ and $w_1$, with the values of $\\mathbf{w}$ evaluated at each iteration superimposed. \n",
    "\n",
    "You should be able to observe the values of $\\mathbf{w}$ slowly moving towards the minimum value of $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBO9q_8trXT6"
   },
   "outputs": [],
   "source": [
    "def get_cost_matrix():\n",
    "    w0_list = np.linspace(-1, 1, 100)\n",
    "    w1_list = np.linspace(-1, 1, 100)\n",
    "\n",
    "    J = np.zeros((len(w0_list),len(w1_list)))\n",
    "    for i in range(len(w0_list)):\n",
    "        for j in range(len(w1_list)):\n",
    "            w0 = w0_list[i]\n",
    "            w1 = w1_list[j]\n",
    "            w[0] = w0\n",
    "            w[1] = w1\n",
    "            ypred = predict(X, w)\n",
    "            J[i,j] = cost_function(ypred, y)\n",
    "    w0ax, w1ax = np.meshgrid(w0_list, w1_list)\n",
    "    return w0ax, w1ax, J\n",
    "  \n",
    "def plot_gradient_descent(w_values):\n",
    "    # Plot the w values at each iteration\n",
    "    plt.scatter(w_values[:,0], w_values[:,1], color='k', s=10)\n",
    "    # Contour map of cost function\n",
    "    w0ax, w1ax, J = get_cost_matrix() \n",
    "    CS = plt.contour(w1ax, w0ax, J, cmap='viridis')\n",
    "    # Add labels to the contours\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "    plt.xlabel('w0')\n",
    "    plt.ylabel('w1')\n",
    "    plt.title('J(w)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8k3F1PBwTRc"
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent(w_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDpAvUthgc7N"
   },
   "source": [
    "## 6. Understanding the performance of a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iyHOebmsgc7N"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# modify predictions such that they equal 1 if p >= 0.5 else 0\n",
    "y_pred = # INSERT CODE HERE\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlVceaDLgc7Q"
   },
   "source": [
    "For a reminder about precision, recall and f1-score: \n",
    "\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTtuEJ7egc7Q"
   },
   "source": [
    "Plot the confusion matrix to visualise the Type 1 and Type 2 errors (False Positive and False Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYxDmXRKgc7R"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    " \n",
    "from itertools import product\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "for i, j in product(range(2), range(2)):\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4r2x6Z5gc7T"
   },
   "source": [
    "Plot a ROC curve and measure the AUC (Area under the Curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "th9s4ak9gc7T"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    " \n",
    "# Return the model probabilities instead of the binarised predictions\n",
    "y_prob = np.matrix([[1-i, i] for i in predict(X, w)])\n",
    "\n",
    "# Calculate the FPR and TPR at various thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y, y_prob[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    " \n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='AUC={:.4f}'.format(roc_auc))\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUASeLmGgc7U"
   },
   "source": [
    "**Questions:** \n",
    "1. Explain the main elements of the ROC curve. Sketch a ROC curve for the best (100% accurate) and worst case (random predictions) models.\n",
    "2. What does an AUC measures. What is the expected AUC value for the best and worst case models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1VXK_AjVgc7U"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvQbijqVgc7W"
   },
   "source": [
    "## Part B: Object Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bbl32mENgc7W"
   },
   "source": [
    "## 1. Introduction: \n",
    "In OOP we are concerned with creating *objects* with properties rather than methods that implement actions. \n",
    "\n",
    "**Questions**\n",
    "\n",
    "* What is the `__init__` method for and that is it for?\n",
    "\n",
    "** INSERT ANSWER HERE **\n",
    "\n",
    "* What does `self` refer to?\n",
    "\n",
    "** INSERT ANSWER HERE **\n",
    "\n",
    "* What are arguments and keyword arguments? Why would you choose one or the other? How are they implemented in python?\n",
    "\n",
    "** INSERT ANSWER HERE **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWTRyu96gc7W"
   },
   "source": [
    "## 2. class LogisticRegression:\n",
    "Below you will implement the logistic regression methods above as part of a class object.\n",
    "\n",
    "Things to consider when implementing your solution:\n",
    "\n",
    "* What parameters should be set on initialisation? \n",
    "* What parameters should be considered class attributes?\n",
    "* What should this class automatically do when initialised?\n",
    "* Write a short docstring explaining each method (there are many ways to do this, see example for numpy convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gquK-G2Zgc7X"
   },
   "outputs": [],
   "source": [
    "def safe_log(x):\n",
    "    return np.where(x==0, np.log(x+1e-10), np.log(x))\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.n_iterations=1000        \n",
    "        # assign the input arguments to class attributes\n",
    "        # INSERT CODE HERE\n",
    "        \n",
    "        # \"private\" methods are preceded with an underscore in the name\n",
    "        # in other programming languages such functions are treated differently\n",
    "        # in Python we use this convention as an indication to the user that they are\n",
    "        # intended to be called by other methods in the class and not from outside of the class\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Implements the sigmoid function\n",
    "                           1\n",
    "        sigmoid(x) =  -------------\n",
    "                      1 + exp(-x)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float or iterable\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or iterable\n",
    "        \"\"\"\n",
    "        return 1./(1 + np.exp(-x))\n",
    "        \n",
    "    def _calc_dJ_dw(self, X, y, w):\n",
    "        # INSERT CODE HERE\n",
    "\n",
    "    def _initialise_w(self, n_features):\n",
    "        # INSERT CODE HERE\n",
    "\n",
    "    def _update_w(self, w, alpha, dJ_dw):\n",
    "        # INSERT CODE HERE\n",
    "        \n",
    "    def cost_function(self, y, ypred):\n",
    "        # INSERT CODE HERE\n",
    "\n",
    "    def predict_proba(self, y_pred):\n",
    "        # INSERT CODE HERE\n",
    "        \n",
    "    def predict(self, X, w):\n",
    "        # INSERT CODE HERE\n",
    "\n",
    "    def fit(self, X, y, n_iterations=100, alpha=0.01, verbose=False):\n",
    "        # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jmx6yUPzgc7a"
   },
   "outputs": [],
   "source": [
    "# Edit the commands below as required\n",
    "\n",
    "# Initialse Model\n",
    "??? = LogisticRegression(???)\n",
    "\n",
    "# Fit Model\n",
    "??? = ???.fit(???)\n",
    "\n",
    "# Make Predictions\n",
    "??? = ???.predict(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression using sklearn Package (easy ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(200, 2, 2, 0, weights=[.5, .5], random_state=15)\n",
    "clf = LogisticRegression().fit(X[:100], y[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                      vmin=0, vmax=1)\n",
    "ax_c = f.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "ax.scatter(X[:,0], X[:, 1], c=y[:], s=50,\n",
    "           cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n",
    "           edgecolor=\"white\", linewidth=1)\n",
    "\n",
    "ax.set(aspect=\"equal\",\n",
    "       xlim=(-5, 5), ylim=(-5, 5),\n",
    "       xlabel=\"$X_1$\", ylabel=\"$X_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure illustrates the decision boundary for separating between class=1 (prob >= 0.5) and class=0 (prob < 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Logistic Regession.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FQhu31QFllAZ"
   },
   "source": [
    "# Linear Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZ6b0FzGKITn"
   },
   "source": [
    "Aims:\n",
    "\n",
    "* Implement batch gradient descent for a simple linear regression model\n",
    "* Review matrix dot products with `numpy`\n",
    "* Simple plots with `matplotlib.pyplot`\n",
    "\n",
    "You will need to fill in parts that say \n",
    "    # INSERT CODE HERE\n",
    "    \n",
    "where possible the exercises will indicate the expected output for you to check your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-w2Tx2wliJg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "29JmiAxbWpVi"
   },
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0ST6vQzl1CZ"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''Load data for the exercise.\n",
    "    \n",
    "    Returns:\n",
    "    X : 2d numpy array\n",
    "    y : 1d numpy array\n",
    "    '''\n",
    "    # Loads data from CSV - do not change this !!\n",
    "    data = np.loadtxt('regression.csv', delimiter=',', skiprows=1)\n",
    "    X, y = data[:,[1,0]], data[:, 2]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 608,
     "status": "error",
     "timestamp": 1534262807422,
     "user": {
      "displayName": "Chris Dancel",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "109195381970588174472"
     },
     "user_tz": -60
    },
    "id": "0rGKDflLl6h3",
    "outputId": "8df5b399-e576-43cb-8f45-ac7840f07c4b"
   },
   "outputs": [],
   "source": [
    "# Get the data. X and y are numpy arrays.\n",
    "X, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uk9ds9KTKITy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the dimensions of the matrix X in the form (nrows, ncolumns)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY1LobavKIT0"
   },
   "source": [
    "`X` is a two-dimensional array with 5000 rows and 2 columns. In other words, our feature dataset contains 5000 examples and has 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXEfb59mKIT1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the dimensions of the matrix y\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aieSObBeKIT4"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    (5000,)\n",
    "    \n",
    "`y` is a one-dimensional array with length 5000. This is our target data - the values we wish to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afTmi0cJKIT5"
   },
   "source": [
    "## 2. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMUkdDJeKIT5"
   },
   "source": [
    "Let's begin by taking a look at the first few rows of data.\n",
    "\n",
    "Hint: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#arrays-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFtZqh2OKIT5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.32674476],\n",
       "       [ 1.        ,  2.56008454],\n",
       "       [ 1.        , -0.61138525],\n",
       "       [ 1.        , -0.89783007],\n",
       "       [ 1.        ,  0.63293182]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows of X\n",
    "\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvlSVW0NKIT9"
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    array([[ 1.        ,  0.32674476],\n",
    "           [ 1.        ,  2.56008454],\n",
    "           [ 1.        , -0.61138525],\n",
    "           [ 1.        , -0.89783007],\n",
    "           [ 1.        ,  0.63293182]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAPU_-fRKIT9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.65056972,  42.12063856,  -7.70932904, -12.49118658,\n",
       "         8.86824912])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 5 rows of y\n",
    "\n",
    "y[0:5:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iytnm0LuKIUA"
   },
   "source": [
    "Expected output: \n",
    "\n",
    "    array([ 14.65056972,  42.12063856,  -7.70932904, -12.49118658,\n",
    "             8.86824912])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DT3oIOAKKIUB"
   },
   "source": [
    "It is often useful to make some simple plots to visualise the data. We will be using the `matplotlib` library to make our plots.\n",
    "\n",
    "Hints:\n",
    "* Slicing with numpy: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#arrays-indexing\n",
    "* Scatter plots: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.scatter.html\n",
    "* Histograms: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0gvuWAu2Ek5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x782753ce48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatter plot of the first column of X vs y\n",
    "# You will need to \"slice\" X to get just the values in the first column\n",
    "# As a bonus, add some labels to your axes\n",
    "\n",
    "xvalues = X[::,0]\n",
    "plt.scatter(xvalues,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SG4AQTSKIUE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7827527b38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a scatter plot of the second column of X vs y\n",
    "\n",
    "# INSERT CODE HERE\n",
    "\n",
    "xvalues = X[::,1]\n",
    "plt.scatter(xvalues,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aIfGONd12KP"
   },
   "source": [
    "** Question: Is this dataset appropriate for a linear regression model? Explain. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "    # INSERT ANSWER HERE\n",
    "    Yes, there is a clear linear relationship in the second scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWGZ3L1hqRgj"
   },
   "source": [
    "## 2. Defining the model and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83dbk3enoUQE"
   },
   "source": [
    "### Linear Regression\n",
    "\n",
    "We will model the data using linear regression.\n",
    "The predictions $\\hat{y}^i$ are given by the linear model\n",
    "\n",
    "$\\hat{y}^i=w_0x^i_0 + w_1x^i_1$.   $\\quad\\quad\\quad(1)$\n",
    "\n",
    "Notation explained:\n",
    "\n",
    "* The superscript $i$ denotes the $i$-th example (or row) in the dataset.\n",
    "* $x_0$ and $x_1$ are the two features (or columns) in `X`\n",
    "* $w_0$ and $w_1$ are the parameters of the model and indicate the dependence of $\\hat{y}$ on $x_0$ and $x_1$, respectively.\n",
    "\n",
    "**Bias term**\n",
    "\n",
    "You may have noticed that all the values $x_0^i=1$. In this case, $w_0$ is effectively a bias term and we could write the model as simply\n",
    "\n",
    "$\\hat{y}^i=w_0 + w_1x^i_1$,   $\\quad\\quad\\quad(2)$\n",
    "\n",
    "but to allow more elegant matrix operations we will state $x^i_0$ explicitly.\n",
    "\n",
    "Hence, as a matrix dot product, we can write\n",
    "\n",
    "$\\hat{y}=X\\cdot\\mathbf{w}$,   $\\quad\\quad\\quad(3)$\n",
    "\n",
    "where $X$ is a matrix with shape `(n samples, n features = 2)` and $\\mathbf{w}$ is the weights vector $[w_0, w_1]$.\n",
    "\n",
    "**Question:** What are the dimensions of $\\hat{y}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1,2])\n",
    "dot_prod = np.dot(X,w)\n",
    "np.shape(dot_prod)\n",
    "\n",
    "#dimensions of y^ are one dimensional array of length n samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avSDpnt3KIUK"
   },
   "source": [
    "Define a `predict(X,w)` method that calculates the linear regression model for a given input of X and w.\n",
    "\n",
    "Hint: https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0b4bQp2LprH1"
   },
   "outputs": [],
   "source": [
    "# Define a function that calculates the linear regression model. This will come in useful later.\n",
    "def predict(X, w):\n",
    "    '''The linear regression model as defined in Equation (1).\n",
    "    It takes X and w as inputs and returns a 1d-array of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    X : 2d-array, shape=(n_samples,n_features)\n",
    "    w : 1d-array, shape=(n_features,)\n",
    "\n",
    "    Returns:\n",
    "    ypred : 1d-array, shape=(n_samples,)\n",
    "    '''\n",
    "    # Check that the number of features in X is equal to the number features in the weights vector\n",
    "    assert X.shape[1] == len(w)\n",
    "\n",
    "    ypred = np.dot(X,w)\n",
    "    \n",
    "    # Check that the number of predictions made is equal to the number of samples in X\n",
    "    assert len(ypred) == X.shape[0]\n",
    "\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXVf9JQDKIUQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32872134,  1.89205918, -0.32796967, -0.52848105,  0.54305227])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with these example values of w\n",
    "w = np.array([0.1, 0.7])\n",
    "predict(X[:5], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3m-v47_KIUT"
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    array([ 0.32872134,  1.89205918, -0.32796967, -0.52848105,  0.54305227])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KxafyWDGKIUT"
   },
   "source": [
    "### Cost function\n",
    "In order to find the values of $\\mathbf{w}$ that best represent the data we need to define a cost function that quantifies how far away our estimate of $\\hat{y}$ differs from the true values $y$.\n",
    "\n",
    "For linear regression the cost function to be minimised is the **RMSE** between the true $y^i$ and predicted $\\hat{y}^i$ values:\n",
    "\n",
    "$J(\\mathbf{w})=\\frac{1}{2m}\\sum^m_{i=1}(\\hat{y}^i-y^i)^2$. $\\quad\\quad\\quad(4)$\n",
    "\n",
    "* We sum the squared error over all samples from $i=1$ to $i=m$, where $m$ is the number of samples in the data.\n",
    "* The additional multiplication by half $\\frac{1}{2}$ compared to a normal RMSE equation is a convention that is there for convenience when we compute gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIFeo2S8KIUV"
   },
   "source": [
    "Define a `cost_function(ypred, y)` method that calculates the cost function in Equation (4).\n",
    "\n",
    "Hint: https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63bxkO5pZmjk"
   },
   "outputs": [],
   "source": [
    "def cost_function(ypred, y):\n",
    "    '''The cost function J(w) as defined in Equation (4).\n",
    "\n",
    "    Parameters:\n",
    "    ypred : 1d-array, y values predicted by model \n",
    "    y : 1d-array, true y values\n",
    "\n",
    "    Returns:\n",
    "    float, J (the cost)\n",
    "    '''\n",
    "    # m is the number of samples\n",
    "    m = len(ypred) # INSERT CODE HERE\n",
    "    errors = ypred - y\n",
    "    errors_squared = errors ** 2\n",
    "    sum_errors = np.sum(errors_squared)\n",
    "    \n",
    "    # J is the cost\n",
    "    J = sum_errors / (2*m)\n",
    "    # Check that J is a scalar\n",
    "    assert J.shape == ()\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT3vGbZbqV4F"
   },
   "source": [
    "## 3. Batch Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeODKjetp7m3"
   },
   "source": [
    "The goal of gradient descent is to find the values of $w_0$ and $w_1$ that **minimise** the cost function $J(\\mathbf{w})$.\n",
    "\n",
    "The minimum is found by evaluating the _gradient_ $\\frac{dJ}{d\\mathbf{w}}$ at a given $\\mathbf{w}$ value, and taking a small step, the _learning rate_ $\\alpha$, along it.\n",
    "\n",
    "There are many different gradient descent algorithms. Here we will implement the **batch gradient descent algorithm**. In batch gradient descent, we will use all the examples in the sample at each iteration to update $w_0$ and $w_1$.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "1. Initialise $\\mathbf{w}=[w_0, w_1]$ with random values.\n",
    "2. Calculate the gradients $\\frac{dJ}{dw_0}$, $\\frac{dJ}{dw_1}$ at this point.\n",
    "3. Update the value of $\\mathbf{w}$ using the update equations\n",
    "\n",
    "  $w_0 := w_0 - \\alpha \\frac{dJ}{dw_0}$, $\\quad\\quad\\quad(5)$\n",
    "\n",
    "  $w_1 := w_1 - \\alpha \\frac{dJ}{dw_1}$. $\\quad\\quad\\quad(6)$\n",
    "4. Repeat steps 2 and 3 until the change in $\\mathbf{w}$ is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYIuczBPqgeZ"
   },
   "source": [
    "Let's implement each step in turn.\n",
    "\n",
    "### Step 1: Initialise w with random values\n",
    "\n",
    "Hint: https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.rand.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3yNbgp4X6m8"
   },
   "outputs": [],
   "source": [
    "def initialise_w(n_features):\n",
    "    '''Initialise the weights vector w with random values.\n",
    "\n",
    "    Parameters:\n",
    "    n_features : int, number of features \n",
    "\n",
    "    Returns:\n",
    "    w : 1d-array\n",
    "    '''\n",
    "    # Set a seed so we get predictable values\n",
    "    np.random.seed(1)\n",
    "\n",
    "    w = np.random.rand(2,)\n",
    "    \n",
    "    # Check that w has the right shape\n",
    "    assert w.shape == (n_features,)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9BbDyWkYBK1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0.4170,0.7203]\n",
      "yhat[0]=0.6524\n",
      "Cost=144.0347\n"
     ]
    }
   ],
   "source": [
    "# Test the initialise_w method\n",
    "w = initialise_w(2)\n",
    "\n",
    "print('w = [{:.4f},{:.4f}]'.format(w[0], w[1]))\n",
    "\n",
    "ypred = predict(X, w)\n",
    "print('yhat[0]={:.4f}'.format(ypred[0]))\n",
    "\n",
    "cost = cost_function(ypred, y)\n",
    "#print(cost.shape)\n",
    "print('Cost={:.4f}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B57BEsyjibMO"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "w = [0.4170,0.7203]\n",
    "yhat[0]=0.6524\n",
    "Cost=144.0347\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9P16h0KLqk4J"
   },
   "source": [
    "### Step 2: Calculate the gradient of J\n",
    "\n",
    "Recalling Equations (1) and (4):\n",
    "\n",
    "$\\hat{y}^i=w_0x^i_0 + w_1x^i_1$.   $\\quad\\quad\\quad(1)$\n",
    "\n",
    "$J(\\mathbf{w})=\\frac{1}{2m}\\sum^m_{i=1}(\\hat{y}^i-y^i)^2$. $\\quad\\quad\\quad(4)$\n",
    "\n",
    "Evaluate the gradients $\\frac{dJ}{dw_0}$ and $\\frac{dJ}{dw_1}$. You may need some pen and paper for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbMrqNF8KIUh"
   },
   "source": [
    "Implement `calc_dJ/dw0(X, y, w)` and `calc_dJ/dw1(X, y, w)`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dJ/dw0 = 1/m * SUM((ypred - y)*x0)\n",
    "dJ/dw1 = 1/m * SUM((ypred - y)*x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32674476,  2.56008454, -0.61138525, ...,  1.08004807,\n",
       "       -1.28042935,  0.1943843 ])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HTQB3isZ5Rn"
   },
   "outputs": [],
   "source": [
    "def calc_dJ_dw0(X, y, w):\n",
    "    '''Function to calculate dJ/dw0\n",
    "\n",
    "    Parameters:\n",
    "    X : 2d-array, feature matrix\n",
    "    y : 1d-array, true y values\n",
    "    w : 1d-array, linear regression model weights\n",
    "\n",
    "    Returns:\n",
    "    dJ_dw0 : float\n",
    "    '''\n",
    "    # Number of samples\n",
    "    m = len(y) # INSERT CODE HERE\n",
    "    \n",
    "    ypred = predict(X,w) # INSERT CODE HERE\n",
    "    \n",
    "    dJ_dw0 = ( m ** -1 ) * np.sum( ( ypred - y ) * X[:,0]  )   # INSERT CODE HERE\n",
    "    \n",
    "    return dJ_dw0\n",
    "\n",
    "\n",
    "def calc_dJ_dw1(X, y, w):\n",
    "    '''Function to calculate dJ/dw1\n",
    "\n",
    "    Parameters:\n",
    "    X : 2d-array, feature matrix\n",
    "    y : 1d-array, true y values\n",
    "    w : 1d-array, linear regression model weights\n",
    "\n",
    "    Returns:\n",
    "    dJ_dw1 : float\n",
    "    '''\n",
    "    # Number of samples\n",
    "    m = len(y) # INSERT CODE HERE\n",
    "    \n",
    "    ypred = predict(X,w) # INSERT CODE HERE\n",
    "    \n",
    "    dJ_dw1 = ( m ** -1 ) * np.sum( ( ypred - y ) * X[:,1]  )   # INSERT CODE HERE\n",
    "\n",
    "    return dJ_dw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_rrvVbiiGXf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ/dw0=-2.2698\n",
      "dJ/dw1=-15.9842\n"
     ]
    }
   ],
   "source": [
    "# Check the derivatives of J wrt w0 and w1\n",
    "w = initialise_w(2)\n",
    "dJ_dw0 = calc_dJ_dw0(X, y, w)\n",
    "dJ_dw1 = calc_dJ_dw1(X, y, w)\n",
    "\n",
    "print('dJ/dw0={:.4f}'.format(dJ_dw0))\n",
    "print('dJ/dw1={:.4f}'.format(dJ_dw1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQGCNvcAjF2y"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "dJ/dw0=-2.2698\n",
    "dJ/dw1=-15.9842\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lA3KOwPr4bWw"
   },
   "source": [
    "A much more elegation solution is to use `np.dot` to perform a matrix multiplication and evaluate $dJ/dw_0$ and $dJ/dw_1$ in a single operation to return the vector \n",
    "\n",
    "[$dJ/dw_0$, $dJ/dw_1$]. \n",
    "\n",
    "Examine the equations you wrote out for the derivatives and try to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TN_6Odvs21-w"
   },
   "outputs": [],
   "source": [
    "def calc_dJ_dw(X, y, w):\n",
    "    '''Function to calculate dJ/dw\n",
    "\n",
    "    Parameters:\n",
    "    X : 2d-array, feature matrix\n",
    "    y : 1d-array, true y values\n",
    "    w : 1d-array, linear regression model weights\n",
    "\n",
    "    Returns:\n",
    "    dJ_dw : 1d-array\n",
    "    '''\n",
    "    # Number of samples\n",
    "    m = len(y)\n",
    "    \n",
    "    ypred = predict(X,w)\n",
    "    \n",
    "    dJ_dw = ( m ** -1 ) * np.dot(( ypred - y ) , X )  # INSERT CODE HERE\n",
    "\n",
    "    # Check that dJ_dw has the right shape\n",
    "    assert dJ_dw.shape == w.shape\n",
    "\n",
    "    return dJ_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-1tUHT828Do"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJ/dw = [dJ/dw_0, dJ/dw_1] = [ -2.2697622  -15.98422692]\n"
     ]
    }
   ],
   "source": [
    "# Check the calc_dJ_dw method\n",
    "dJ_dw = calc_dJ_dw(X, y, w)\n",
    "print('dJ/dw = [dJ/dw_0, dJ/dw_1] =', dJ_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuX4gZIVKIUu"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    dJ/dw = [dJ/dw_0, dJ/dw_1] = [ -2.2697622  -15.98422692]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_8NAa40qoPq"
   },
   "source": [
    "### Step 3: Update the value of w\n",
    "\n",
    "With the gradients in hand we can update the value of $\\mathbf{w}$ using the update equations\n",
    "\n",
    "  $w_0 := w_0 - \\alpha \\frac{dJ}{dw_0}$, $\\quad\\quad\\quad(5)$\n",
    "\n",
    "  $w_1 := w_1 - \\alpha \\frac{dJ}{dw_1}$. $\\quad\\quad\\quad(6)$\n",
    "  \n",
    "This improves our estimate of $\\mathbf{w}$ by taking a step of size $\\alpha$ \"downhill\" towards the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lkrcNe-iCeI"
   },
   "outputs": [],
   "source": [
    "def update_w(w, alpha, dJ_dw):\n",
    "    '''Update the weights vector w.\n",
    "    \n",
    "    Parameters:\n",
    "    w : 1d-array, weights vector\n",
    "    alpha : float, learning rate\n",
    "    dJ_dw : 1d-array, gradients vector\n",
    "    \n",
    "    Returns:\n",
    "    new_w : 1d-array, updated weights vector\n",
    "    '''    \n",
    "    new_w = w - (alpha * dJ_dw  )# INSERT CODE HERE\n",
    "    \n",
    "    # Check the dimensions of new_w\n",
    "    assert new_w.shape == w.shape\n",
    "    \n",
    "    return new_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Buj0_3ORr7Ys"
   },
   "source": [
    "Let's try taking one iteration of batch gradient descent. \n",
    "\n",
    "As a reminder, the algorithm goes as follows:\n",
    "1. Initialise $\\mathbf{w}=[w_0, w_1]$ with random values.\n",
    "2. Calculate the gradients $\\frac{dJ}{dw_0}$, $\\frac{dJ}{dw_1}$ at this point.\n",
    "3. Update the value of $\\mathbf{w}$ using the update equations $w_0 := w_0 - \\alpha \\frac{dJ}{dw_0}$, $w_1 := w_1 - \\alpha \\frac{dJ}{dw_1}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z52wGIe3YRum"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new w=[0.43971963 0.88016676]\n"
     ]
    }
   ],
   "source": [
    "# Initialise w to random value\n",
    "w = initialise_w(2)  #should probably make this dynamic# INSERT CODE HERE\n",
    "\n",
    "# Calculate the gradient\n",
    "dJ_dw = calc_dJ_dw(X, y, w) # INSERT CODE HERE\n",
    "\n",
    "# Set the value of alpha to 0.01\n",
    "alpha = 0.01\n",
    "\n",
    "# Update w\n",
    "new_w = update_w(w, alpha, dJ_dw) # INSERT CODE HERE\n",
    "\n",
    "print('new w={}'.format(new_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd9Q3pyam_i9"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "new w=[0.43971963 0.88016676]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FTg2Ur2qqlC"
   },
   "source": [
    "Calculate the cost $J$ at the updated value of $\\mathbf{w}$ which we obtained after one step of gradient descent. \n",
    "\n",
    "**Question**: Do you expect it to increase or decrease? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "    Should decrease as we have taken a step towards the minimum. Could have increased if we have chosen too large an alpha though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q8g7Aaw_YSHM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost: 144.0347\n",
      "Cost after one step of gradient descent: 141.4412\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cost at new_w\n",
    "\n",
    "new_cost = cost_function(predict(X, new_w), y)# INSERT CODE HERE\n",
    "\n",
    "print('Initial Cost: {:.4f}'.format(cost))\n",
    "print('Cost after one step of gradient descent: {:.4f}'.format(new_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOO300TvnEX5"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Initial Cost: 144.0347\n",
    "Cost after one step of gradient descent: 141.4412\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-o1u1HBxqvDu"
   },
   "source": [
    "### Step 4: Repeat\n",
    "\n",
    "We have a made a small improvement in the estimation of the parameters for our linear regression model. \n",
    "\n",
    "In order to find the best estimate of $\\mathbf{w}$ we will need to do many iterations of gradient descent to get to the minimum of $J(\\mathbf{w})$. \n",
    "\n",
    "Let's put everything together into a single \"fit\" function that takes X and y as inputs and iterates the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "228tEsDzY646"
   },
   "outputs": [],
   "source": [
    "def fit(X, y, n_iterations=100, alpha=0.01, verbose=False):\n",
    "    '''Fit linear regression model to data X, y.\n",
    "    \n",
    "    Parameters:\n",
    "    X : 2d-array, feature matrix shape=(m, n_features)\n",
    "    y : 1d-array, targets\n",
    "    n_iterations : int, number of iterations of gradient descent\n",
    "    alpha : float, learning rate\n",
    "    verbose : bool, prints the cost every 10 iterations\n",
    "    \n",
    "    Returns:\n",
    "    w : nd-array, final weights matrix shape=(n_features,)\n",
    "    cost_values : 1d-array, cost at each iteration shape=(n_iterations,)\n",
    "    w_values : nd-array, weights at each iteration shape=(n_iterations, n_features)\n",
    "    '''\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Step 1: Initialise w at a random point\n",
    "    w = initialise_w(n_features)# INSERT CODE HERE\n",
    "    \n",
    "    # We are going to save the values of the cost and w at each iteration for later analysis\n",
    "    cost_values = [] \n",
    "    w_values = [] \n",
    "    iteration = []\n",
    "    \n",
    "    # Repeat n_iterations times\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        # Step 2: Calculate the gradient \n",
    "        # INSERT CODE HERE\n",
    "        dJ_dw = calc_dJ_dw(X, y, w)\n",
    "        \n",
    "        # Step 3: Update w\n",
    "        # INSERT CODE HERE\n",
    "        w = update_w(w, alpha, dJ_dw)\n",
    "        \n",
    "        # Calculate the cost \n",
    "        # INSERT CODE HERE\n",
    "        cost = cost_function(predict(X, w), y)\n",
    "\n",
    "        if verbose and i % 10 == 0:\n",
    "          print('Iteration {}: Cost={:.6f}'.format(i, cost))\n",
    "\n",
    "        # Save the values of the cost and w after each iteration\n",
    "        cost_values.append(cost)\n",
    "        w_values.append(w)\n",
    "        iteration.append(i)\n",
    "        \n",
    "    cost_values = np.array(cost_values)\n",
    "    w_values = np.array(w_values)\n",
    "    iteration = np.array(iteration)\n",
    "    \n",
    "    return w, cost_values, w_values, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHYQU6HUKIVA"
   },
   "source": [
    "Let's try fitting X and y with the values\n",
    "\n",
    "    alpha=0.01\n",
    "    n_iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmp2XL43mKsx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost=141.441227\n",
      "Iteration 10: Cost=118.168016\n",
      "Iteration 20: Cost=99.111526\n",
      "Iteration 30: Cost=83.507731\n",
      "Iteration 40: Cost=70.731044\n",
      "Iteration 50: Cost=60.269229\n",
      "Iteration 60: Cost=51.702865\n",
      "Iteration 70: Cost=44.688527\n",
      "Iteration 80: Cost=38.945014\n",
      "Iteration 90: Cost=34.242076\n",
      "Final value of w=[ 1.83227168 10.88093585]\n"
     ]
    }
   ],
   "source": [
    "w, cost_values, w_values ,iteration = fit(X, y, alpha=0.01, n_iterations=100, verbose=True)\n",
    "\n",
    "print('Final value of w={}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAC3sLwgsQdH"
   },
   "source": [
    "Expected output:\n",
    "```\n",
    "Final value of w=[ 1.83227168 10.88093585]\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising gradient descent\n",
    "\n",
    "### a. Cost as a function of gradient descent iteration\n",
    "\n",
    "It is often useful to plot the cost after each iteration of gradient descent to check that it is decreasing. (We will see in future weeks that there is a lot of other useful information here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXY6qhtWmMvy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7828aadbe0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cost at each iteration of gradient descent\n",
    "# INSERT CODE HERE\n",
    "\n",
    "plt.scatter(iteration,cost_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBO9q_8trXT6"
   },
   "outputs": [],
   "source": [
    "def get_cost_matrix():\n",
    "    w0_list = np.linspace(-7.7, 11.7, 100)\n",
    "    w1_list = np.linspace(0, 32, 100)\n",
    "\n",
    "    J = np.zeros((len(w0_list),len(w1_list)))\n",
    "    for i in range(len(w0_list)):\n",
    "        for j in range(len(w1_list)):\n",
    "            w0 = w0_list[i]\n",
    "            w1 = w1_list[j]\n",
    "            ypred = predict(X, [w0, w1])\n",
    "            J[i,j] = cost_function(ypred, y)\n",
    "    w0ax, w1ax = np.meshgrid(w0_list, w1_list)\n",
    "    return w0ax, w1ax, J\n",
    "  \n",
    "def plot_gradient_descent(w_values):\n",
    "    # Plot the w values at each iteration\n",
    "    plt.scatter(w_values[:,0], w_values[:,1], color='k', s=10)\n",
    "    # Contour map of cost function\n",
    "    w0ax, w1ax, J = get_cost_matrix() \n",
    "    CS = plt.contour(w0ax, w1ax, J, cmap='viridis')\n",
    "    # Add labels to the contours\n",
    "    plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "    plt.xlabel('w0')\n",
    "    plt.ylabel('w1')\n",
    "    plt.title('J(w)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8k3F1PBwTRc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_gradient_descent(w_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2daEVxbVmgg8"
   },
   "source": [
    "## Optimizing gradient descent\n",
    "\n",
    "The true value of $\\mathbf{w}$ is `w = [2.7, 16.82365791084919]` where the cost is approximately `J=12.99`.\n",
    "\n",
    "**Question: How could you get a better estimate of w? (Bonus: test your ideas!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fd-UMXe_1CX1"
   },
   "source": [
    "### b. Cost as a function of $\\mathbf{w}$\n",
    "\n",
    "Below is some code that will plot the cost function $J$ as a function of $w_0$ and $w_1$, with the values of $\\mathbf{w}$ evaluated at each iteration superimposed. \n",
    "\n",
    "You should be able to observe the values of $\\mathbf{w}$ slowly moving towards the minimum value of $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "    \n",
    "    After 100 iterations it looks like we are still moving towards the minimum, and havent leveled out much yet.\n",
    "    Running for 500 uterations shows that we level out around 250-300 iteration mark, so sould use 300 iterations for this\n",
    "    \n",
    "    The steps shown on the graph all look quite small, so we can probably safely increase Alpha to take larger steps.\n",
    "    Playing around with this actually seems to show alpha of 1 gets us more or less straight to the minimum\n",
    "    Going too big causes issues. At alpha = 2 this bounces about and diverges\n",
    "    at alpha = 1.5 it seems to converge, but bounces about on the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gSNF2tBxtfC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear Regression - Workbook.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
